{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "import math\n",
    "import numpy as np\n",
    "import iris\n",
    "import iris.analysis\n",
    "import iris.coord_categorisation\n",
    "import iris.plot\n",
    "import dask.bag as db\n",
    "import basic_info\n",
    "import shutil # Used for copying files at the end of the script to the new folders\n",
    "\n",
    "baseline_start_year = 1995\n",
    "baseline_end_year = 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the max HI (max tas, min rh) and mean HI (mean tas, mean rh) to calculate scaling factor for future projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_day_month_season_year_dims(cube):\n",
    "    # Add day of month\n",
    "    try:\n",
    "        cube.remove_coord('day_of_month')\n",
    "    except iris.exceptions.CoordinateNotFoundError:\n",
    "        pass\n",
    "    iris.coord_categorisation.add_day_of_month(cube, 'time', name='day_of_month')\n",
    "    \n",
    "    # Add month\n",
    "    try:\n",
    "        cube.remove_coord('month')\n",
    "    except iris.exceptions.CoordinateNotFoundError:\n",
    "        pass\n",
    "    iris.coord_categorisation.add_month(cube, 'time', name='month')\n",
    "    \n",
    "    # Add season year\n",
    "    try:\n",
    "        cube.remove_coord('season_year')\n",
    "    except iris.exceptions.CoordinateNotFoundError:\n",
    "        pass\n",
    "    iris.coord_categorisation.add_season_year(cube, 'time', name='season_year')\n",
    "    \n",
    "    return cube\n",
    "\n",
    "def hourly_to_daily_max(cube):\n",
    "    # Add day, month, and season year dimensions\n",
    "    add_day_month_season_year_dims(cube)\n",
    "    \n",
    "    # Aggregate by day of month, month, and season year\n",
    "    daily_max = cube.aggregated_by(['day_of_month', 'month', 'season_year'], iris.analysis.MAX)\n",
    "    \n",
    "    return daily_max\n",
    "\n",
    "def hourly_to_daily_mean(cube):\n",
    "    # Add day, month, and season year dimensions\n",
    "    add_day_month_season_year_dims(cube)\n",
    "    \n",
    "    # Aggregate by day of month, month, and season year\n",
    "    daily_means = cube.aggregated_by(['day_of_month', 'month', 'season_year'], iris.analysis.MEAN)\n",
    "    \n",
    "    return daily_means\n",
    "\n",
    "def hourly_to_daily_min(cube):\n",
    "    # Add day, month, and season year dimensions\n",
    "    add_day_month_season_year_dims(cube)\n",
    "    \n",
    "    # Aggregate by day of month, month, and season year\n",
    "    daily_min = cube.aggregated_by(['day_of_month', 'month', 'season_year'], iris.analysis.MIN)\n",
    "    \n",
    "    return daily_min\n",
    "\n",
    "def extract_monthly_cubes(cube):\n",
    "    monthly_cubes = []\n",
    "    # Extract unique years\n",
    "    years = set([cell.point.year for cell in cube.coord('time').cells()])\n",
    "    # Iterate over each year\n",
    "    for year in sorted(years):  # Sorting to ensure chronological order\n",
    "        # Iterate over each month\n",
    "        for month in range(1, 13):\n",
    "            # Apply constraint for specific year and month\n",
    "            month_constraint = iris.Constraint(time=lambda cell: cell.point.year == year and cell.point.month == month)\n",
    "            month_cube = cube.extract(month_constraint)\n",
    "            if month_cube:  # Check if the cube is not empty\n",
    "                monthly_cubes.append(month_cube)\n",
    "    return monthly_cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_cube = iris.load_cube(basic_info.tas_filepath)\n",
    "rh_cube = iris.load_cube(basic_info.rh_filepath)\n",
    "\n",
    "tas_cube = tas_cube.extract(iris.Constraint(time=lambda cell: baseline_start_year <= cell.point.year <= baseline_end_year))\n",
    "rh_cube = rh_cube.extract(iris.Constraint(time=lambda cell: baseline_start_year <= cell.point.year <= baseline_end_year))\n",
    "\n",
    "tas_cube.convert_units('celsius')\n",
    "tas_cube.data.fill_value = -9999\n",
    "\n",
    "rh_cube.data.fill_value = -9999\n",
    "\n",
    "tas_daily_max = hourly_to_daily_max(tas_cube)\n",
    "rh_daily_min = hourly_to_daily_min(rh_cube)\n",
    "\n",
    "tas_daily_mean = hourly_to_daily_mean(tas_cube)\n",
    "rh_daily_mean = hourly_to_daily_mean(rh_cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Lu and Romps for both HI mean and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lu and Romps\n",
    "# Version 1.1 released by Yi-Chuan Lu on February 23, 2023.\n",
    "#    Release 1.1 accommodates old Python 2 installations that\n",
    "#    interpret some constants as integers instead of reals.\n",
    "# Version 1.0 released by Yi-Chuan Lu on May 18, 2022.\n",
    "# \n",
    "# When using this code, please cite:\n",
    "# \n",
    "# @article{20heatindex,\n",
    "#   Title   = {Extending the Heat Index},\n",
    "#   Author  = {Yi-Chuan Lu and David M. Romps},\n",
    "#   Journal = {Journal of Applied Meteorology and Climatology},\n",
    "#   Year    = {2022},\n",
    "#   Volume  = {61},\n",
    "#   Number  = {10},\n",
    "#   Pages   = {1367--1383},\n",
    "#   Year    = {2022},\n",
    "# }\n",
    "#\n",
    "# This headindex function returns the Heat Index in Kelvin. The inputs are:\n",
    "# - T, the temperature in Kelvin\n",
    "# - RH, the relative humidity, which is a value from 0 to 1\n",
    "# - show_info is an optional logical flag. If true, the function returns the physiological state.\n",
    "\n",
    "# Thermodynamic parameters\n",
    "Ttrip = 273.16       # K\n",
    "ptrip = 611.65       # Pa\n",
    "E0v   = 2.3740e6     # J/kg\n",
    "E0s   = 0.3337e6     # J/kg\n",
    "rgasa = 287.04       # J/kg/K \n",
    "rgasv = 461.         # J/kg/K \n",
    "cva   = 719.         # J/kg/K\n",
    "cvv   = 1418.        # J/kg/K \n",
    "cvl   = 4119.        # J/kg/K\n",
    "cvs   = 1861.        # J/kg/K\n",
    "cpa   = cva + rgasa\n",
    "cpv   = cvv + rgasv\n",
    "\n",
    "# The saturation vapor pressure\n",
    "def pvstar(T):\n",
    "    if T == 0.0:\n",
    "        return 0.0\n",
    "    elif T<Ttrip:\n",
    "        return ptrip * (T/Ttrip)**((cpv-cvs)/rgasv) * math.exp( (E0v + E0s -(cvv-cvs)*Ttrip)/rgasv * (1./Ttrip - 1./T) )\n",
    "    else:\n",
    "        return ptrip * (T/Ttrip)**((cpv-cvl)/rgasv) * math.exp( (E0v       -(cvv-cvl)*Ttrip)/rgasv * (1./Ttrip - 1./T) )\n",
    "\n",
    "# The latent heat of vaporization of water\n",
    "def Le(T):\n",
    "    return (E0v + (cvv-cvl)*(T-Ttrip) + rgasv*T)\n",
    "\n",
    "# Thermoregulatory parameters\n",
    "sigma       = 5.67e-8                     # W/m^2/K^4 , Stefan-Boltzmann constant\n",
    "epsilon     = 0.97                        #           , emissivity of surface, steadman1979\n",
    "M           = 70.5                        # kg        , mass of average US adults, fryar2018 # Changed to average SA adult (Worlddata.info)\n",
    "H           = 1.69                        # m         , height of average US adults, fryar2018 # Changed to average SA adult (Worlddata.info)\n",
    "A           = 0.202*(M**0.425)*(H**0.725) # m^2       , DuBois formula, parson2014\n",
    "cpc         = 3492.                       # J/kg/K    , specific heat capacity of core, gagge1972\n",
    "C           = M*cpc/A                     #           , heat capacity of core\n",
    "r           = 124.                        # Pa/K      , Zf/Rf, steadman1979\n",
    "Q           = 180.                        # W/m^2     , metabolic rate per skin area, steadman1979\n",
    "phi_salt    = 0.9                         #           , vapor saturation pressure level of saline solution, steadman1979\n",
    "Tc          = 310.                        # K         , core temperature, steadman1979\n",
    "Pc          = phi_salt * pvstar(Tc)       #           , core vapor pressure\n",
    "L           = Le(310.)                    #           , latent heat of vaporization at 310 K\n",
    "p           = 1.013e5                     # Pa        , atmospheric pressure\n",
    "eta         = 1.43e-6                     # kg/J      , \"inhaled mass\" / \"metabolic rate\", steadman1979\n",
    "Pa0         = 1.6e3                       # Pa        , reference air vapor pressure in regions III, IV, V, VI, steadman1979\n",
    "\n",
    "# Thermoregulatory functions\n",
    "def Qv(Ta,Pa): # respiratory heat loss, W/m^2\n",
    "    return  eta * Q *(cpa*(Tc-Ta)+L*rgasa/(p*rgasv) * ( Pc-Pa ) )\n",
    "# print(\"Defining Qf\")\n",
    "def Zs(Rs): # mass transfer resistance through skin, Pa m^2/W\n",
    "    return (52.1 if Rs == 0.0387 else 6.0e8 * Rs**5)\n",
    "def Ra(Ts,Ta): # heat transfer resistance through air, exposed part of skin, K m^2/W\n",
    "    hc      = 17.4\n",
    "    phi_rad = 0.85\n",
    "    hr      = epsilon * phi_rad * sigma* (Ts**2 + Ta**2)*(Ts + Ta)\n",
    "    return 1./(hc+hr)\n",
    "def Ra_bar(Tf,Ta): # heat transfer resistance through air, clothed part of skin, K m^2/W\n",
    "    hc      = 11.6\n",
    "    phi_rad = 0.79\n",
    "    hr      = epsilon * phi_rad * sigma* (Tf**2 + Ta**2)*(Tf + Ta)\n",
    "    return 1./(hc+hr)\n",
    "def Ra_un(Ts,Ta): # heat transfer resistance through air, when being naked, K m^2/W\n",
    "    hc      = 12.3\n",
    "    phi_rad = 0.80\n",
    "    hr      = epsilon * phi_rad * sigma* (Ts**2 + Ta**2)*(Ts + Ta)\n",
    "    return 1./(hc+hr)\n",
    "\n",
    "Za     = 60.6/17.4  # Pa m^2/W, mass transfer resistance through air, exposed part of skin\n",
    "Za_bar = 60.6/11.6  # Pa m^2/W, mass transfer resistance through air, clothed part of skin\n",
    "Za_un  = 60.6/12.3  # Pa m^2/W, mass transfer resistance through air, when being naked\n",
    "\n",
    "# tolerance and maximum iteration for the root solver \n",
    "tol     = 1e-8\n",
    "tolT    = 1e-8\n",
    "maxIter = 100\n",
    "\n",
    "# Given air temperature and relative humidity, returns the equivalent variables \n",
    "def find_eqvar(Ta,RH):\n",
    "    if Ta > 500:\n",
    "        return [-9999,-9999,-9999,-9999,-9999]\n",
    "    elif RH > 500:\n",
    "        return [-9999,-9999,-9999,-9999,-9999] # I added these past 3 lines to handle the case when RH data is missing\n",
    "    else:\n",
    "        Pa    = RH*pvstar(Ta) #         , air vapor pressure\n",
    "        Rs    = 0.0387        # m^2K/W  , heat transfer resistance through skin\n",
    "        phi   = 0.84          #         , covering fraction\n",
    "        dTcdt = 0.            # K/s     , rate of change in Tc\n",
    "        m     = (Pc-Pa)/(Zs(Rs)+Za)\n",
    "        m_bar = (Pc-Pa)/(Zs(Rs)+Za_bar)\n",
    "        Ts = solve(lambda Ts: (Ts-Ta)/Ra(Ts,Ta)     + (Pc-Pa)/(Zs(Rs)+Za)     - (Tc-Ts)/Rs, max(0.,min(Tc,Ta)-Rs*abs(m)),     max(Tc,Ta)+Rs*abs(m),    tol,maxIter)\n",
    "        Tf = solve(lambda Tf: (Tf-Ta)/Ra_bar(Tf,Ta) + (Pc-Pa)/(Zs(Rs)+Za_bar) - (Tc-Tf)/Rs, max(0.,min(Tc,Ta)-Rs*abs(m_bar)), max(Tc,Ta)+Rs*abs(m_bar),tol,maxIter)\n",
    "        flux1 = Q-Qv(Ta,Pa)-(1.-phi)*(Tc-Ts)/Rs                   # C*dTc/dt when Rf=Zf=\\inf\n",
    "        flux2 = Q-Qv(Ta,Pa)-(1.-phi)*(Tc-Ts)/Rs - phi*(Tc-Tf)/Rs  # C*dTc/dt when Rf=Zf=0\n",
    "        if (flux1 <= 0.) : # region I\n",
    "            eqvar_name = \"phi\"\n",
    "            phi = 1.-(Q-Qv(Ta,Pa))*Rs/(Tc-Ts)\n",
    "            Rf  = float('inf')\n",
    "        elif (flux2 <=0.) : # region II&III\n",
    "            eqvar_name = \"Rf\"\n",
    "            Ts_bar = Tc - (Q-Qv(Ta,Pa))*Rs/phi + (1./phi -1.)*(Tc-Ts)\n",
    "            Tf = solve(lambda Tf: (Tf-Ta)/Ra_bar(Tf,Ta) + (Pc-Pa)*(Tf-Ta)/((Zs(Rs)+Za_bar)*(Tf-Ta)+r*Ra_bar(Tf,Ta)*(Ts_bar-Tf)) - (Tc-Ts_bar)/Rs, Ta,Ts_bar,tol,maxIter)\n",
    "            Rf = Ra_bar(Tf,Ta)*(Ts_bar-Tf)/(Tf-Ta)\n",
    "        else: # region IV,V,VI\n",
    "            Rf = 0.\n",
    "            flux3 =  Q-Qv(Ta,Pa)-(Tc-Ta)/Ra_un(Tc,Ta)-(phi_salt*pvstar(Tc)-Pa)/Za_un\n",
    "            if (flux3 < 0.) : # region IV,V\n",
    "                Ts = solve(lambda Ts: (Ts-Ta)/Ra_un(Ts,Ta)+(Pc-Pa)/(Zs((Tc-Ts)/(Q-Qv(Ta,Pa)))+Za_un)-(Q-Qv(Ta,Pa)),0.,Tc,tol,maxIter)\n",
    "                Rs = (Tc-Ts)/(Q-Qv(Ta,Pa))\n",
    "                eqvar_name = \"Rs\"\n",
    "                Ps = Pc - (Pc-Pa)* Zs(Rs)/( Zs(Rs)+Za_un)\n",
    "                if (Ps > phi_salt * pvstar(Ts)):  # region V\n",
    "                    Ts = solve( lambda Ts : (Ts-Ta)/Ra_un(Ts,Ta) + (phi_salt*pvstar(Ts)-Pa)/Za_un -(Q-Qv(Ta,Pa)), 0.,Tc,tol,maxIter)\n",
    "                    Rs = (Tc-Ts)/(Q-Qv(Ta,Pa))\n",
    "                    eqvar_name = \"Rs*\"\n",
    "            else: # region VI\n",
    "                Rs = 0.\n",
    "                eqvar_name = \"dTcdt\"\n",
    "                dTcdt = (1./C)* flux3\n",
    "    return [eqvar_name,phi,Rf,Rs,dTcdt]\n",
    "\n",
    "# given the equivalent variable, find the Heat Index\n",
    "def find_T(eqvar_name,eqvar):\n",
    "    if (eqvar_name == \"phi\"):\n",
    "        T = solve(lambda T: find_eqvar(T,1.)[1]-eqvar,0.,240.,tolT,maxIter)\n",
    "        region = 'I'\n",
    "    elif (eqvar_name == \"Rf\"):\n",
    "        T = solve(lambda T: find_eqvar(T,min(1.,Pa0/pvstar(T)))[2]-eqvar,230.,300.,tolT,maxIter)\n",
    "        region = ('II' if Pa0>pvstar(T) else 'III')\n",
    "    elif (eqvar_name == \"Rs\" or eqvar_name == \"Rs*\"):\n",
    "        T = solve(lambda T: find_eqvar(T,Pa0/pvstar(T))[3]-eqvar,295.,350.,tolT,maxIter)\n",
    "        region = ('IV' if eqvar_name == \"Rs\" else 'V')\n",
    "    else:\n",
    "        T = solve(lambda T: find_eqvar(T,Pa0/pvstar(T))[4]-eqvar,340.,1000.,tolT,maxIter)\n",
    "        region = 'VI'\n",
    "    return T, region\n",
    "\n",
    "# combining the two functions find_eqvar and find_T\n",
    "def heatindex(Ta,RH,show_info=False):\n",
    "    dic = {\"phi\":1,\"Rf\":2,\"Rs\":3,\"Rs*\":3,\"dTcdt\":4}\n",
    "    eqvars = find_eqvar(Ta,RH)\n",
    "    if eqvars[0] == -9999:\n",
    "        return -9999\n",
    "    else:\n",
    "        T, region = find_T(eqvars[0],eqvars[dic[eqvars[0]]])\n",
    "        if (Ta == 0.): T = 0.\n",
    "        if (show_info==True):\n",
    "            if region=='I':\n",
    "                print(\"Region I, covering (variable phi)\")\n",
    "                print(\"Clothing fraction is \"+ str(round(eqvars[1],3)))\n",
    "            elif region=='II':\n",
    "                print(\"Region II, clothed (variable Rf, pa = pvstar)\")\n",
    "                print(\"Clothing thickness is \"+ str(round((eqvars[2]/16.7)*100.,3))+\" cm\")\n",
    "            elif region=='III':\n",
    "                print(\"Region III, clothed (variable Rf, pa = pref)\")\n",
    "                print(\"Clothing thickness is \"+ str(round((eqvars[2]/16.7)*100.,3))+\" cm\")\n",
    "            elif region=='IV':\n",
    "                kmin = 5.28               # W/K/m^2     , conductance of tissue\n",
    "                rho  = 1.0e3              # kg/m^3      , density of blood\n",
    "                c    = 4184.              # J/kg/K      , specific heat of blood\n",
    "                print(\"Region IV, naked (variable Rs, ps < phisalt*pvstar)\")\n",
    "                print(\"Blood flow is \" + str(round(( (1./eqvars[3] - kmin)*A/(rho*c) ) *1000.*60.,3))+\" l/min\")\n",
    "            elif region=='V':\n",
    "                kmin = 5.28               # W/K/m^2     , conductance of tissue\n",
    "                rho  = 1.0e3              # kg/m^3      , density of blood\n",
    "                c    = 4184.              # J/kg/K      , specific heat of blood\n",
    "                print(\"Region V, naked dripping sweat (variable Rs, ps = phisalt*pvstar)\")\n",
    "                print(\"Blood flow is \" + str(round(( (1./eqvars[3] - kmin)*A/(rho*c) ) *1000.*60.,3))+\" l/min\")\n",
    "            else:\n",
    "                print(\"Region VI, warming up (dTc/dt > 0)\")\n",
    "                print(\"dTc/dt = \"+ str(round(eqvars[4]*3600.,6))+ \" K/hour\")\n",
    "    return T\n",
    "\n",
    "def solve(f,x1,x2,tol,maxIter):\n",
    "    a  = x1\n",
    "    b  = x2\n",
    "    fa = f(a)\n",
    "    fb = f(b)\n",
    "    if fa*fb>0.:\n",
    "        raise SystemExit('wrong initial interval in the root solver')\n",
    "        return None\n",
    "    else:\n",
    "        for i in range(maxIter):\n",
    "            c  = (a+b)/2.\n",
    "            fc = f(c)\n",
    "            if fb*fc > 0. :\n",
    "                b  = c\n",
    "                fb = fc\n",
    "            else:\n",
    "                a  = c\n",
    "                fa = fc   \n",
    "            if abs(a-b) < tol:\n",
    "                return c\n",
    "            if i == maxIter-1:\n",
    "                print()\n",
    "                raise SystemExit('reaching maximum iteration in the root solver')\n",
    "                return None\n",
    "\n",
    "def heat_index_lu_and_romps_vectorized_pre_processing(tas_cube, rh_cube):\n",
    "    tas_cube_converted = tas_cube.copy()\n",
    "    tas_cube_converted.convert_units('kelvin')\n",
    "    tas_cube_converted.data = np.ma.masked_where(tas_cube_converted.data == -9999, tas_cube_converted.data)\n",
    "    tas_cube_converted.data.set_fill_value(-9999)\n",
    "    monthly_tas_cubes = extract_monthly_cubes(tas_cube_converted)\n",
    "\n",
    "    rh_cube_converted = rh_cube.copy()\n",
    "    rh_cube_converted.data = rh_cube_converted.data / 100\n",
    "    rh_cube_converted.data = np.ma.masked_where(rh_cube_converted.data == -9999, rh_cube_converted.data)\n",
    "    rh_cube_converted.data.set_fill_value(-9999) \n",
    "    monthly_rh_cubes = extract_monthly_cubes(rh_cube_converted)\n",
    "\n",
    "    return monthly_tas_cubes, monthly_rh_cubes\n",
    "\n",
    "def heat_index_lu_and_romps_vectorized(monthly_tas_cube, monthly_rh_cube):\n",
    "    heat_index_vectorized = np.vectorize(heatindex)(monthly_tas_cube.data, monthly_rh_cube.data)\n",
    "    monthly_tas_cube.data = heat_index_vectorized\n",
    "    return monthly_tas_cube\n",
    "\n",
    "def save_cube(cube, path):\n",
    "    iris.save(cube, path)\n",
    "    print(f'Saved {path}')\n",
    "\n",
    "def process_and_save(monthly_tas_cube, monthly_rh_cube, file_path):\n",
    "    result = heat_index_lu_and_romps_vectorized(monthly_tas_cube, monthly_rh_cube)\n",
    "    save_cube(result, file_path)\n",
    "\n",
    "def heat_index_lu_and_romps_vectorized_processing(monthly_tas_cubes, monthly_rh_cubes, metric_name):\n",
    "    lu_romps_files = f'{os.getcwd()}/lu_romps_files/{metric_name}'\n",
    "    if not os.path.exists(lu_romps_files):\n",
    "        os.makedirs(lu_romps_files)\n",
    "    \n",
    "    print('creating heat_index_cube')\n",
    "    \n",
    "    tasks = []\n",
    "    for i, (monthly_tas_cube, monthly_rh_cube) in enumerate(zip(monthly_tas_cubes, monthly_rh_cubes)):\n",
    "        file_path = f'{lu_romps_files}/Month_{i}.nc'\n",
    "        if not os.path.isfile(file_path):\n",
    "            tasks.append((monthly_tas_cube, monthly_rh_cube, file_path))\n",
    "            print(f'Processing Month_{i}')\n",
    "    \n",
    "    print('creating dask bag')\n",
    "    bag = db.from_sequence(tasks)\n",
    "\n",
    "    print('computing')\n",
    "    bag.map(lambda x: process_and_save(*x)).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lu_and_romps_files(metric_name):\n",
    "    lu_and_romps_cubelist = iris.cube.CubeList([])\n",
    "    lu_and_romps_files_dir_path = f'{os.getcwd()}/lu_romps_files/{metric_name}'\n",
    "    lu_and_romps_files = os.listdir(lu_and_romps_files_dir_path)\n",
    "\n",
    "    for file in lu_and_romps_files:\n",
    "        lu_and_romps_cubelist.append(iris.load_cube(f'{lu_and_romps_files_dir_path}/{file}'))\n",
    "        print(f'Loaded {file} cube')\n",
    "\n",
    "    heat_index_lu_and_romps_cube = iris.cube.CubeList.concatenate(lu_and_romps_cubelist)\n",
    "    save_cube(heat_index_lu_and_romps_cube, f'{os.getcwd()}/lu_romps_files/{metric_name}_concatenated_{metric_name}.nc')\n",
    "    del heat_index_lu_and_romps_cube\n",
    "    print(f'Computed {metric_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEAN HI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tas_cubes, mean_rh_cubes = heat_index_lu_and_romps_vectorized_pre_processing(tas_daily_mean, rh_daily_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_index_lu_and_romps_vectorized_processing(mean_tas_cubes, mean_rh_cubes, 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_lu_and_romps_files('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAX HI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tas_cubes, min_rh_cubes = heat_index_lu_and_romps_vectorized_pre_processing(tas_daily_max, rh_daily_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_index_lu_and_romps_vectorized_processing(max_tas_cubes, min_rh_cubes, 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_lu_and_romps_files('max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCALING FACTOR (ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the \"difference cube\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the historical maximum and mean cubes\n",
    "max_cube = iris.load_cube(f'{os.getcwd()}/lu_romps_files/max_concatenated_max.nc')\n",
    "\n",
    "mean_cube = iris.load_cube(f'{os.getcwd()}/lu_romps_files/mean_concatenated_mean.nc')\n",
    "\n",
    "# Calculate the scaling factor cube\n",
    "scaling_factor_cube = max_cube / mean_cube\n",
    "\n",
    "# Add 'day_of_year' auxiliary coordinate to the scaling factor cube\n",
    "iris.coord_categorisation.add_day_of_year(scaling_factor_cube, 'time', name='day_of_year')\n",
    "\n",
    "# Extract the day of the year coordinates from the cube\n",
    "day_of_year = scaling_factor_cube.coord('day_of_year')\n",
    "\n",
    "# Select only unique values of the day of the year to calculate the mean for each day\n",
    "days_of_year = np.unique(day_of_year.points)\n",
    "\n",
    "# Calculate the mean scaling factor for each day of the year\n",
    "daily_scaling_factors = []\n",
    "for day in days_of_year:\n",
    "    # Extract data for the current day of the year\n",
    "    day_constraint = iris.Constraint(day_of_year=day)\n",
    "    daily_data = scaling_factor_cube.extract(day_constraint)\n",
    "    \n",
    "    # Calculate the mean scaling factor for the current day\n",
    "    daily_mean_scaling_factor = daily_data.collapsed('time', iris.analysis.MEAN)\n",
    "    daily_scaling_factors.append(daily_mean_scaling_factor)\n",
    "\n",
    "# Combine the daily scaling factors into a new cube that contains the average values per day of the year for each grid cell\n",
    "combined_scaling_factor_cube = iris.cube.CubeList(daily_scaling_factors).merge_cube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_estimated_daily_maximum(future_data_path, scaling_factor_cube, output_path):\n",
    "    # Load the projected future data cube\n",
    "    future_data_cube = iris.load_cube(future_data_path)\n",
    "    future_data_cube.convert_units('Kelvin')\n",
    "\n",
    "    # Add 'day_of_year' auxiliary coordinate to the future data cube\n",
    "    iris.coord_categorisation.add_day_of_year(future_data_cube, 'time', name='day_of_year')\n",
    "\n",
    "    # Check and match coordinate systems (added after MUCH trial and error getting an error message about mismatched coordinate systems)\n",
    "    future_cs = future_data_cube.coord_system()\n",
    "    scaling_cs = scaling_factor_cube.coord_system()\n",
    "\n",
    "    if future_cs != scaling_cs:\n",
    "        if future_cs is not None and scaling_cs is None:\n",
    "            scaling_factor_cube.coord('latitude').coord_system = future_cs\n",
    "            scaling_factor_cube.coord('longitude').coord_system = future_cs\n",
    "        elif future_cs is None and scaling_cs is not None:\n",
    "            future_data_cube.coord('latitude').coord_system = scaling_cs\n",
    "            future_data_cube.coord('longitude').coord_system = scaling_cs\n",
    "\n",
    "    # Regrid the 'scaling factor cube' to match the spatial resolution of the future data cube\n",
    "    regridded_scaling_factor_cube = scaling_factor_cube.regrid(future_data_cube, iris.analysis.Linear())\n",
    "\n",
    "    # Expand the 'scaling factor cube' to match the time dimension of the future data cube\n",
    "    # Get the day_of_year points from the future data cube\n",
    "    future_day_of_year_points = future_data_cube.coord('day_of_year').points\n",
    "\n",
    "    # Create a new array with the same shape as the future data cube\n",
    "    expanded_scaling_factor_data = np.empty_like(future_data_cube.data)\n",
    "\n",
    "    # Fill the new array with the scaling factor data repeated for each day of the year\n",
    "    for i, day_of_year in enumerate(future_day_of_year_points):\n",
    "        # Extract each day from the regridded 'scaling factor' cube\n",
    "        day_constraint = iris.Constraint(day_of_year=day_of_year)\n",
    "        day_data = regridded_scaling_factor_cube.extract(day_constraint).data\n",
    "        expanded_scaling_factor_data[i, :, :] = day_data\n",
    "\n",
    "    # Create a new cube with the expanded data\n",
    "    expanded_scaling_factor_cube = future_data_cube.copy(data=expanded_scaling_factor_data)\n",
    "\n",
    "    # Multiply the future data cube by the expanded scaling factor cube\n",
    "    scaled_future_data = future_data_cube * expanded_scaling_factor_cube\n",
    "\n",
    "    # Save the scaled future data cube to a new file\n",
    "    iris.save(scaled_future_data, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_folder = basic_info.projections_data_folder + '/scaled'\n",
    "os.makedirs(scaled_folder, exist_ok=True)\n",
    "\n",
    "# Baslines\n",
    "## CMIP6\n",
    "calculate_estimated_daily_maximum(\n",
    "    basic_info.projections_cmip6_baseline_heat_index,\n",
    "    combined_scaling_factor_cube,\n",
    "    scaled_folder + '/CMIP6_ERA5_heat_index_1995_2014.nc'\n",
    ")\n",
    "## CORDEX\n",
    "calculate_estimated_daily_maximum(\n",
    "    basic_info.projections_cordex_baseline_heat_index,\n",
    "    combined_scaling_factor_cube,\n",
    "    scaled_folder + '/CORDEX_ERA5_heat_index_1995_2014.nc'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated to work with the lists of future data cubes that I will use\n",
    "# Note to self: had to create the \"prefixes\" list as otherwise just putting {filepath} in the function call would not work and create a permission error\n",
    "def calculate_estimated_daily_maximum(future_data_cube, scaling_factor_cube, output_dir, output_file_name):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert units of the future data cube\n",
    "    future_data_cube.convert_units('Kelvin')\n",
    "\n",
    "    # Add 'day_of_year' auxiliary coordinate to the future data cube\n",
    "    iris.coord_categorisation.add_day_of_year(future_data_cube, 'time', name='day_of_year')\n",
    "\n",
    "    # Check and align coordinate systems (added after MUCH trial and error getting an error message about mismatched coordinate systems)\n",
    "    future_cs = future_data_cube.coord_system()\n",
    "    scaling_cs = scaling_factor_cube.coord_system()\n",
    "\n",
    "    if future_cs != scaling_cs:\n",
    "        if future_cs is not None and scaling_cs is None:\n",
    "            scaling_factor_cube.coord('latitude').coord_system = future_cs\n",
    "            scaling_factor_cube.coord('longitude').coord_system = future_cs\n",
    "        elif future_cs is None and scaling_cs is not None:\n",
    "            future_data_cube.coord('latitude').coord_system = scaling_cs\n",
    "            future_data_cube.coord('longitude').coord_system = scaling_cs\n",
    "\n",
    "    # Regrid the 'scaling factor cube' to match the spatial resolution of the future data cube\n",
    "    regridded_scaling_factor_cube = scaling_factor_cube.regrid(future_data_cube, iris.analysis.Linear())\n",
    "\n",
    "    # Expand the 'scaling factor cube' to match the time dimension of the future data cube\n",
    "    # Get the day_of_year points from the future data cube\n",
    "    future_day_of_year_points = future_data_cube.coord('day_of_year').points\n",
    "\n",
    "    # Create a new array with the same shape as the future data cube\n",
    "    expanded_scaling_factor_data = np.empty_like(future_data_cube.data)\n",
    "\n",
    "    # Fill the new array with the scaling factor data repeated for each day of the year\n",
    "    for i, day_of_year in enumerate(future_day_of_year_points):\n",
    "        # Extract each day from the regridded 'scaling factor' cube\n",
    "        day_constraint = iris.Constraint(day_of_year=day_of_year)\n",
    "        day_data = regridded_scaling_factor_cube.extract(day_constraint).data\n",
    "        expanded_scaling_factor_data[i, :, :] = day_data\n",
    "\n",
    "    # Create a new cube with the 'expanded' data\n",
    "    expanded_scaling_factor_cube = future_data_cube.copy(data=expanded_scaling_factor_data)\n",
    "\n",
    "    # Multiply the future data cube by the expanded scaling factor cube\n",
    "    scaled_future_data = future_data_cube * expanded_scaling_factor_cube\n",
    "\n",
    "    # Save the scaled future data cube to a new file\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "    iris.save(scaled_future_data, output_file_path)\n",
    "\n",
    "def load_and_process_projection_cubes(filepath, scaling_factor_cube, output_dir, prefix):\n",
    "    '''\n",
    "    Load and process the projection cubes from the given filepaths \n",
    "    and save the scaled cubes to the output directory\n",
    "    '''\n",
    "    files = []\n",
    "    try:\n",
    "        files = os.listdir(filepath)\n",
    "        print(f\"Files in {filepath} loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading files in {filepath}: {e}\")\n",
    "    for i, file in enumerate(files):\n",
    "        try:\n",
    "            cube = iris.load_cube(f'{filepath}/{file}')\n",
    "            cube.convert_units('Kelvin')\n",
    "            output_file_name = f\"{prefix}_scaled_cube_{i}.nc\"\n",
    "            calculate_estimated_daily_maximum(cube, scaling_factor_cube, output_dir, output_file_name)\n",
    "            del cube  # Delete the cube to free up memory\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing cube from {file}: {e}\")\n",
    "\n",
    "# Define filepaths and prefixes\n",
    "filepaths = [\n",
    "    f'{basic_info.projections_data_folder}/CMIP6/2041_2060',\n",
    "    f'{basic_info.projections_data_folder}/CMIP6/2080_2099',\n",
    "    f'{basic_info.projections_data_folder}/CORDEX/2041_2060',\n",
    "    f'{basic_info.projections_data_folder}/CORDEX/2080_2099'\n",
    "]\n",
    "\n",
    "prefixes = [\n",
    "    'CMIP6_2041_2060',\n",
    "    'CMIP6_2080_2099',\n",
    "    'CORDEX_2041_2060',\n",
    "    'CORDEX_2080_2099'\n",
    "]\n",
    "\n",
    "# Load and process projection cubes\n",
    "scaling_factor_cube = combined_scaling_factor_cube  \n",
    "output_dir = f'{basic_info.projections_data_folder}/scaled'\n",
    "\n",
    "for filepath, prefix in zip(filepaths, prefixes):\n",
    "    load_and_process_projection_cubes(filepath, scaling_factor_cube, output_dir, prefix)\n",
    "\n",
    "f = open('basic_info.py', 'a')\n",
    "f.write(f\"\\nscaled_projections_folder = '{output_dir}\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the cubes into folders for further analysis\n",
    "scaled_files = os.listdir(output_dir)\n",
    "for file in scaled_files:\n",
    "    if 'CMIP6' in file:\n",
    "        if '2041' in file:\n",
    "            shutil.move(f'{output_dir}/{file}', f'{output_dir}/CMIP6_2041_2060/{file}')\n",
    "        elif '2080' in file:\n",
    "            shutil.move(f'{output_dir}/{file}', f'{output_dir}/CMIP6_2080_2099/{file}')\n",
    "    elif 'CORDEX' in file:\n",
    "        if '2041' in file:\n",
    "            shutil.move(f'{output_dir}/{file}', f'{output_dir}/CORDEX_2041_2060/{file}')\n",
    "        elif '2080' in file:\n",
    "            shutil.move(f'{output_dir}/{file}', f'{output_dir}/CORDEX_2080_2099/{file}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
